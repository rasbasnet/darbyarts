const fs = require('fs');
const path = require('path');

const agents = require('./ai-bots.json');

const outputPath = path.resolve(__dirname, '../public/robots.txt');
const timestamp = new Date().toISOString();

const normalizedAgents = Array.from(
  new Set(
    agents
      .map((agent) => (typeof agent === 'string' ? agent.trim() : ''))
      .filter(Boolean)
  )
).sort((a, b) => a.localeCompare(b, undefined, { sensitivity: 'base' }));

const header = [
  '# Generated by scripts/generate-robots.js',
  `# Last updated: ${timestamp}`,
  '# Allow all standard crawlers',
  'User-agent: *',
  'Disallow:'
];

const disallowRules = normalizedAgents.map(
  (agent) => `User-agent: ${agent}\nDisallow: /`
);

const contents = `${header.join('\n')}\n\n${disallowRules.join('\n\n')}\n`;

fs.mkdirSync(path.dirname(outputPath), { recursive: true });
fs.writeFileSync(outputPath, contents, 'utf8');

const relativePath = path.relative(process.cwd(), outputPath);
console.log(`Generated ${relativePath}`);
